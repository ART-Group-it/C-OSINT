{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HYdqwvZ_soRI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-actwFfKs1dh"
   },
   "outputs": [],
   "source": [
    "df_surface = pd.read_csv(\"surface_web_Market.csv\")\n",
    "\n",
    "df_dark = pd.read_csv(\"DarkWeb_Covid_Market.csv\")\n",
    "\n",
    "df_surface['target'] = [0]*len(df_surface)\n",
    "df_dark['target'] = [1]*len(df_dark)\n",
    "\n",
    "list_df = [df_surface, df_dark]\n",
    "df = pd.concat(list_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "37ke4cOIwG_d",
    "outputId": "96d44f12-c547-4100-e6cf-6e964d3acb04"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Main_Class</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lucira Check It Single-Use COVID-19 Test, The ...</td>\n",
       "      <td>Market</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lucira Check It Single-Use COVID-19 Test. 3.9 ...</td>\n",
       "      <td>Market</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lucira Check. 556 ratings Currently unavailable.</td>\n",
       "      <td>Market</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lucira Check It Single-Use — Ages 18+.</td>\n",
       "      <td>Market</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CLINITEST Rapid Covid-19 Antigen Self-Test: Co...</td>\n",
       "      <td>Market</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Main_Class  target\n",
       "0  Lucira Check It Single-Use COVID-19 Test, The ...     Market       0\n",
       "1  Lucira Check It Single-Use COVID-19 Test. 3.9 ...     Market       0\n",
       "2   Lucira Check. 556 ratings Currently unavailable.     Market       0\n",
       "3             Lucira Check It Single-Use — Ages 18+.     Market       0\n",
       "4  CLINITEST Rapid Covid-19 Antigen Self-Test: Co...     Market       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POzTYrZcGiGY",
    "outputId": "801f660c-019e-4466-961d-8ce2d526575d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df[\"Main_Class\"].unique()\n",
    "df[\"target\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "PaXL3IgwGuhe",
    "outputId": "09995601-d595-4445-a401-a567c21abe81"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Main_Class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>630</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>630</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Text  Main_Class\n",
       "target                  \n",
       "0        630         630\n",
       "1        630         630"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.groupby(\"Main_Class\").count()\n",
    "df.groupby(\"target\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "57CfRB_xITkG"
   },
   "outputs": [],
   "source": [
    "df.index=range(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e39XACpO6aOK"
   },
   "source": [
    "**WORD-TOKENIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8BMfSXL7xp0",
    "outputId": "d0153e7f-d727-41a2-ab49-8e6939b83952"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "df['tokenized_sents'] = df.apply(lambda row: nltk.word_tokenize(row[\"Text\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAkTVaIo7mSG"
   },
   "source": [
    "**POS-TAGGIN(PART-OF-SPEECH TAGGING)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCn_4UcV6Zca",
    "outputId": "86a023ed-e188-47b1-ff3d-509ecb570918"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "df['tokenized_sents'] = df['tokenized_sents'].apply(lambda x: pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7zvERFQ8x4v",
    "outputId": "a3e01bc7-daa6-4b79-c80a-b79edd9d6f16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [(Lucira, NNP), (Check, NNP), (It, PRP), (Sing...\n",
       "1       [(Lucira, NNP), (Check, NNP), (It, PRP), (Sing...\n",
       "2       [(Lucira, NNP), (Check, NNP), (., .), (556, CD...\n",
       "3       [(Lucira, NNP), (Check, NNP), (It, PRP), (Sing...\n",
       "4       [(CLINITEST, NNP), (Rapid, NNP), (Covid-19, NN...\n",
       "                              ...                        \n",
       "1255    [(-covid19-, NN), (CERTIFICATES, NNS), (ARE, V...\n",
       "1256    [(CERTIFICATES, NNS), (ARE, VBP), (ISSUED, NNP...\n",
       "1257    [(Covid-19, JJ), (Vaccine, NNP), (Cards, NNP),...\n",
       "1258    [(COVID-19, NNP), (,, ,), (Sputnik, NNP), (V, ...\n",
       "1259    [(COVID, NNP), (VACCINE, NNP), (CARDS, NNP), (...\n",
       "Name: tokenized_sents, Length: 1260, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSEc_f1a9vRK"
   },
   "source": [
    "**TO-LOWER-CASE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7Opduytm9utQ"
   },
   "outputs": [],
   "source": [
    "def listOfLists(L):\n",
    "  newL=[]\n",
    "  for t in L:\n",
    "    newL.append(list(t))\n",
    "  return newL\n",
    "\n",
    "def toLowerCase(L):\n",
    "  for l in L:\n",
    "    l[0]=l[0].lower()\n",
    "  return L\n",
    "\n",
    "df['tokenized_sents']=df['tokenized_sents'].apply(lambda x: listOfLists(x))\n",
    "df['tokenized_sents']=df['tokenized_sents'].apply(lambda x: toLowerCase(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kt5II08RVUtT",
    "outputId": "8ab1fab2-ca87-4d17-b014-09d632249600"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[lucira, NNP], [check, NNP], [it, PRP], [sing...\n",
       "1       [[lucira, NNP], [check, NNP], [it, PRP], [sing...\n",
       "2       [[lucira, NNP], [check, NNP], [., .], [556, CD...\n",
       "3       [[lucira, NNP], [check, NNP], [it, PRP], [sing...\n",
       "4       [[clinitest, NNP], [rapid, NNP], [covid-19, NN...\n",
       "                              ...                        \n",
       "1255    [[-covid19-, NN], [certificates, NNS], [are, V...\n",
       "1256    [[certificates, NNS], [are, VBP], [issued, NNP...\n",
       "1257    [[covid-19, JJ], [vaccine, NNP], [cards, NNP],...\n",
       "1258    [[covid-19, NNP], [,, ,], [sputnik, NNP], [v, ...\n",
       "1259    [[covid, NNP], [vaccine, NNP], [cards, NNP], [...\n",
       "Name: tokenized_sents, Length: 1260, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokenized_sents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwcR1woAWPFH"
   },
   "source": [
    "**STOPWORDS-REMOVAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jWJj2Qz6WNoy",
    "outputId": "0b259f60-0627-40ad-f2c7-5bfd9d8ddac9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', ' ', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_list = stopwords.words('english')+list(string.punctuation)+[\" \"]+[\"\"] #noise removal:insieme alle stopwords viene eliminata la punteggiatura\n",
    "\n",
    "print(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FqZ1vC_SPmKQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def removeSublist(a,b):\n",
    "  for el in b:\n",
    "    a.remove(el)\n",
    "\n",
    "#df['tokenized_sents'].apply(lambda x: removeSublist(x,[couple for couple in x if not(set(couple[0]).isdisjoint(stop_list))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zM2IUUx3dTV3"
   },
   "source": [
    "**NOISE-REMOVAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFB5IuSHKu95",
    "outputId": "a90c554f-0ada-4117-d3a9-d58853a665fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "2       None\n",
       "3       None\n",
       "4       None\n",
       "        ... \n",
       "1255    None\n",
       "1256    None\n",
       "1257    None\n",
       "1258    None\n",
       "1259    None\n",
       "Name: tokenized_sents, Length: 1260, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def removeSublist(a,b):\n",
    "  for el in b:\n",
    "    a.remove(el)\n",
    "\n",
    "noises_list=list(string.punctuation)+[\" \"]+[\"\"]\n",
    "\n",
    "df['tokenized_sents'].apply(lambda x: removeSublist(x,[couple for couple in x if not(set(couple[0]).isdisjoint(noises_list))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zssr59m8YSZ5",
    "outputId": "9f293fae-7680-44af-fdc8-9e6c408edb26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[lucira, NNP], [check, NNP], [it, PRP], [test...\n",
       "1       [[lucira, NNP], [check, NNP], [it, PRP], [test...\n",
       "2       [[lucira, NNP], [check, NNP], [556, CD], [rati...\n",
       "3       [[lucira, NNP], [check, NNP], [it, PRP], [—, N...\n",
       "4       [[clinitest, NNP], [rapid, NNP], [antigen, NNP...\n",
       "                              ...                        \n",
       "1255    [[certificates, NNS], [are, VBP], [issued, NNP...\n",
       "1256    [[certificates, NNS], [are, VBP], [issued, NNP...\n",
       "1257    [[vaccine, NNP], [cards, NNP], [european, NNP]...\n",
       "1258    [[sputnik, NNP], [v, NNP], [vaccine, NN], [150...\n",
       "1259    [[covid, NNP], [vaccine, NNP], [cards, NNP], [...\n",
       "Name: tokenized_sents, Length: 1260, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokenized_sents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaGZjTHNd5-P"
   },
   "source": [
    "**LEMMATIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Lz2jito8vkrn"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RR6BMrDvd9Oj",
    "outputId": "b85a8bd4-5956-4ae1-c09e-94d608d378ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizeToken(L):\n",
    "  for i in range(len(L)):\n",
    "    if(get_wordnet_pos(L[i][1])!=0):\n",
    "      L[i][0]=lemmatizer.lemmatize(L[i][0],pos=get_wordnet_pos(L[i][1]))\n",
    "  return L\n",
    "\n",
    "df[\"tokenized_sents\"]=df['tokenized_sents'].apply(lambda x: lemmatizeToken(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "WXl7khOeSOg8",
    "outputId": "0f18ee94-1330-4dad-c163-5caf95705cf4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"going\",pos=get_wordnet_pos(\"VBZ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_FFLgbEsmUB",
    "outputId": "96a13678-d7c7-4c87-b032-8dd40216f69d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[lucira, NNP], [check, NNP], [it, PRP], [test...\n",
       "1       [[lucira, NNP], [check, NNP], [it, PRP], [test...\n",
       "2       [[lucira, NNP], [check, NNP], [556, CD], [rati...\n",
       "3       [[lucira, NNP], [check, NNP], [it, PRP], [—, N...\n",
       "4       [[clinitest, NNP], [rapid, NNP], [antigen, NNP...\n",
       "                              ...                        \n",
       "1255    [[certificate, NNS], [be, VBP], [issued, NNP],...\n",
       "1256    [[certificate, NNS], [be, VBP], [issued, NNP],...\n",
       "1257    [[vaccine, NNP], [card, NNP], [european, NNP],...\n",
       "1258    [[sputnik, NNP], [v, NNP], [vaccine, NN], [150...\n",
       "1259    [[covid, NNP], [vaccine, NNP], [card, NNP], [s...\n",
       "Name: tokenized_sents, Length: 1260, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokenized_sents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1spi9dj_duy4"
   },
   "source": [
    "CONCATENAZIONE DELLE COPPIE TOKEN-TAG OPPURE TOKEN-ENTITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "S_BbodVWEEfl"
   },
   "outputs": [],
   "source": [
    "\n",
    "df[\"tokenized_sents\"]=df[\"tokenized_sents\"].apply(lambda x: ' '.join([el[0]+el[1] for el in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "O5A26nOYGeJ3"
   },
   "outputs": [],
   "source": [
    "df.index=range(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Q9skfLNSIMOX",
    "outputId": "bf9056d5-48fe-4481-fe6f-de8d10982cd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'compliesNNS withIN dataNNS protectionNN requirementNNS andCC workVBZ withIN theDT fhirNNP standardNN'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[970,\"tokenized_sents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YHHuFcYjE3Zr",
    "outputId": "52259322-7b14-4d9a-e341-27def455712b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "#\n",
    "# Create sample set of documents\n",
    "#\n",
    "docs = np.array(df[\"tokenized_sents\"])\n",
    "\n",
    "#\n",
    "# Fit the bag-of-words model\n",
    "#\n",
    "bag = vectorizer.fit_transform(docs)\n",
    "#\n",
    "# Get unique words / tokens found in all the documents. The unique words / tokens represents\n",
    "# the features\n",
    "#\n",
    "#print(vectorizer.get_feature_names())\n",
    "#\n",
    "# Associate the indices with each unique word\n",
    "#\n",
    "#print(vectorizer.vocabulary_)\n",
    "#\n",
    "# Print the numerical feature vector\n",
    "#\n",
    "#print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_pG6K7oG5kG",
    "outputId": "68e28eab-2b04-4bbf-dbd2-a923010cf89b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1260, 2531)\n"
     ]
    }
   ],
   "source": [
    "X=bag.toarray()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qzm7y66AG-jp",
    "outputId": "a3e9c430-978c-4f20-d08e-d902cd913aa8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.93007348e-310],\n",
       "       [6.93007348e-310],\n",
       "       [2.63107980e-316],\n",
       "       ...,\n",
       "       [6.92944663e-310],\n",
       "       [6.92944661e-310],\n",
       "       [6.92944661e-310]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=np.empty((len(df),1))\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RayIHv0dG-g2",
    "outputId": "f5bc1f19-13c7-44d3-d853-6298a3349a70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "authors=df[\"target\"].unique()\n",
    "\n",
    "targets=np.array(df[\"target\"])\n",
    "\n",
    "\n",
    "for i in range (len(df)):\n",
    "  Y[i]=np.where(authors == targets[i])[0][0]\n",
    "\n",
    "Y=Y.astype(\"int\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FY2c08JsHFFr",
    "outputId": "69e8e445-ab0d-4b89-fc24-c30fa1334882"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 2531)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DT8_ex-EHFC_",
    "outputId": "f2a137df-c75f-4b0a-8543-627f431fc139"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "XdgkPpGtHMVr"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X, Y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jl0jUIS_HMS_",
    "outputId": "a913f3b3-e080-4597-9c0b-8e25f7bd17eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(756, 2531) (504, 2531)\n",
      "(756, 1) (504, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape)\n",
    "print(Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d-wT99q3HQ2U",
    "outputId": "80c8e67d-b87b-4d31-e592-5d75c8de3b2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain <class 'numpy.ndarray'>\n",
      "Ytrain <class 'numpy.ndarray'>\n",
      "Xtest <class 'numpy.ndarray'>\n",
      "Ytest <class 'numpy.ndarray'>\n",
      "Xtrain int64\n",
      "Ytrain int64\n",
      "Xtest int64\n",
      "Ytest int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Xtrain\",type(X_train))\n",
    "print(\"Ytrain\",type(Y_train))\n",
    "print(\"Xtest\",type(X_test))\n",
    "print(\"Ytest\",type(Y_test))\n",
    "print(\"Xtrain\",X_train.dtype)\n",
    "print(\"Ytrain\",Y_train.dtype)\n",
    "print(\"Xtest\",X_test.dtype)\n",
    "print(\"Ytest\",Y_test.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_3geQeGHbVM"
   },
   "source": [
    "**Trasformazione degli array numpy in tensori pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "bCmVrw8KHQxe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "Y_train = torch.from_numpy(Y_train).float()\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "Y_test = torch.from_numpy(Y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJEYPSVzHhWi",
    "outputId": "5321e9ab-97d4-49dc-dc9d-daee498b1336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([756, 2531])\n",
      "torch.Size([756, 1])\n",
      "torch.Size([504, 2531])\n",
      "torch.Size([504, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1qzapsGdHt98",
    "outputId": "38f54b71-1e9f-46f6-9ef1-a92e1d7c1218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2531\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "len(X_train[0])\n",
    "\n",
    "input_layer_neurons=len(X_train[0])\n",
    "output_layer_neurons=3\n",
    "\n",
    "print(input_layer_neurons)\n",
    "print(output_layer_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "-wYnwlkxHlOa"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self): \n",
    "    super().__init__()  \n",
    "\n",
    "    self.fc1=nn.Linear(input_layer_neurons,124)  \n",
    "    self.fc2=nn.Linear(124,64)     #Hidden Layer 1\n",
    "    #self.fc3=nn.Linear(64,64)     #Hidden Layer 2\n",
    "    self.fc4=nn.Linear(64,output_layer_neurons)     #Output Layer\n",
    "  \n",
    "  def forward(self,x):\n",
    "    x=fn.relu(self.fc1(x))\n",
    "    x=fn.relu(self.fc2(x))\n",
    "    #x=fn.relu(self.fc3(x))\n",
    "    x=self.fc4(x)\n",
    "\n",
    "    #return x\n",
    "    return fn.log_softmax(x,dim=1)  \n",
    "\n",
    "class Leo(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, 8000)\n",
    "        self.fc2 = torch.nn.Linear(8000, 4000)\n",
    "        self.fc3 = torch.nn.Linear(4000, 2000)\n",
    "        self.synth_sem_linear = nn.Linear(2000, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):        \n",
    "        x = F.dropout(F.relu(self.fc1(x)), p=0.1)\n",
    "        x = F.dropout(F.relu(self.fc2(x)), p=0.1)\n",
    "        x = F.dropout(F.relu(self.fc3(x)), p=0.1)\n",
    "        x_tot = self.synth_sem_linear(x)\n",
    "        return x_tot   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "qxc-YmUGHkuO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=2531, out_features=124, bias=True)\n",
       "  (fc2): Linear(in_features=124, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=Net() #istanziazione della rete neurale\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Nr9vWHjHICxz"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_data = TensorDataset(X_train,Y_train)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(X_test,Y_test)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBRPt0C2H8y-"
   },
   "source": [
    "# FASE DI TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "7yRCFPRSH7-K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  print(\"gpu available\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x7f8e59827c18>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZYjiOFsIO1X",
    "outputId": "fe3eb93b-dda4-4493-b9c3-61b96bae9a1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_function=nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer=optim.Adam(net.parameters(),lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "  for data in train_data:\n",
    "    X, y = data \n",
    "    net.zero_grad()\n",
    "    output=net(X.view(-1,input_layer_neurons).cuda())  \n",
    "    loss=fn.nll_loss(output,y.long().cuda())  \n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "  print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "atp3HWRoIOxw",
    "outputId": "896c4e84-7002-48f2-8d0c-5cf760180f53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  98.6 %\n"
     ]
    }
   ],
   "source": [
    "correct=0 \n",
    "total=0 \n",
    "\n",
    "with torch.no_grad(): \n",
    "  for data in test_data:\n",
    "    X, y = data\n",
    "    output=net(X.view(-1,input_layer_neurons).cuda())\n",
    "    #print(y)\n",
    "    for idx, i in enumerate(output):\n",
    "      #print(torch.argmax(i),y[idx])\n",
    "      if(torch.argmax(i)==y[idx]):\n",
    "        correct+=1\n",
    "      total+=1\n",
    "\n",
    "print(\"ACCURACY: \",round(correct/total,3)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xd30By9lSDdB"
   },
   "source": [
    "10- 72.5%\n",
    "3- 77.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AUTHOR_REC_POS_BOW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
