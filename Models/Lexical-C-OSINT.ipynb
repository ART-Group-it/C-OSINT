{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123456789\n",
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data import get_tokenizer\n",
    "embedding_glove = GloVe(name='6B', dim=300)\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_embeddings(sentences, embedding_glove):\n",
    "    sentences_boe = []\n",
    "    for s in sentences:\n",
    "        boe = torch.zeros(300)\n",
    "        for w in tokenizer(s):\n",
    "            boe += embedding_glove.get_vecs_by_tokens(w)\n",
    "        sentences_boe.append(boe)\n",
    "    return sentences_boe\n",
    "\n",
    "def unplickle_trees(path_tree_file):\n",
    "    print('read DTKs:', path_tree_file)\n",
    "    dt_trees = []\n",
    "    with open(path_tree_file, 'rb') as fr:\n",
    "        try:\n",
    "            while True:\n",
    "                dt_trees.append(pickle.load(fr))\n",
    "        except EOFError:\n",
    "            pass\n",
    "    return [torch.FloatTensor(i) for i in dt_trees]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTarget(df):\n",
    "    target = []\n",
    "    for x in df.target:\n",
    "        if x == 'Forum' or x == 0:\n",
    "            target.append(0)\n",
    "        if x == 'Market' or x == 1:\n",
    "            target.append(1)\n",
    "    return np.array(target)\n",
    "\n",
    "def unplickle_trees(path_tree_file):\n",
    "    print('read DTKs:', path_tree_file)\n",
    "    dt_trees = []\n",
    "    with open(path_tree_file, 'rb') as fr:\n",
    "        try:\n",
    "            while True:\n",
    "                dt_trees.append(pickle.load(fr))\n",
    "        except EOFError:\n",
    "            pass\n",
    "    return [torch.FloatTensor(i) for i in dt_trees]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1_surface = 'surface_web_Market'\n",
    "dataset_tree_surface = 'dtk_trees_surface_web_COVID_Market'\n",
    "\n",
    "\n",
    "dataset1_dark = 'DarkWeb_Covid_Market'\n",
    "dataset_tree_dark = 'dtk_trees_DarkWeb_Covid_Market'\n",
    "\n",
    "\n",
    "df_surface = pd.read_csv(path + dataset1_surface + '.csv')\n",
    "df_dark = pd.read_csv(path + dataset1_dark + '.csv')\n",
    "\n",
    "df_surface['target'] = [0]*len(df_surface)\n",
    "df_dark['target'] = [1]*len(df_dark)\n",
    "\n",
    "list_df = [df_surface, df_dark]\n",
    "df_ = pd.concat(list_df)\n",
    "\n",
    "sentences = df_.Text.values\n",
    "labels = setTarget(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = df_.Text.values\n",
    "sentences_train_boe = bag_of_embeddings(sentences_train,embedding_glove)\n",
    "labels_train = setTarget(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trees = unplickle_trees(path+dataset_tree+'.pkl')\n",
    "\n",
    "trees_surface = unplickle_trees(path+dataset_tree_surface+'.pkl')\n",
    "trees_dark = unplickle_trees(path+dataset_tree_dark+'.pkl')\n",
    "trees = trees_surface + trees_dark\n",
    "\n",
    "print('trees:', len(trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_train = []\n",
    "for s in sentences_train:\n",
    "    for w in tokenizer(s):\n",
    "        vocab_train.append(w.lower())\n",
    "vocab_train = list(set(vocab_train))\n",
    "print('vocab_train size:',len(vocab_train))\n",
    "missing_word = []\n",
    "for w in vocab_train:\n",
    "    if float(embedding_glove.get_vecs_by_tokens(w)[0]) == 0:\n",
    "        missing_word.append(w)\n",
    "print('missing_word size:',len(missing_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "X_train_synt, X_test_synt, target_train_synt, target_test = train_test_split(trees, labels_train, random_state=RANDOM_STATE, test_size=0.3)\n",
    "X_train_sem, X_test_sem, target_train_sem, _ = train_test_split(sentences_train_boe, labels_train, random_state=RANDOM_STATE, test_size=0.3)\n",
    "X_train_synt, X_validation_synt, target_train, target_validation = train_test_split(X_train_synt, target_train_synt, random_state=RANDOM_STATE, test_size=0.1)\n",
    "X_train_sem, X_validation_sem, _, _ = train_test_split(X_train_sem, target_train_sem, random_state=RANDOM_STATE, test_size=0.1)\n",
    "\n",
    "X_train_synt = torch.stack(X_train_synt)\n",
    "X_train_sem = torch.stack(X_train_sem)\n",
    "target_train = torch.tensor(target_train)\n",
    "\n",
    "X_validation_synt = torch.stack(X_validation_synt)\n",
    "X_validation_sem = torch.stack(X_validation_sem)\n",
    "target_validation = torch.tensor(target_validation)\n",
    "\n",
    "X_test_synt = torch.stack(X_test_synt)\n",
    "X_test_sem = torch.stack(X_test_sem)\n",
    "target_test = torch.tensor(target_test)\n",
    "\n",
    "X_train_synt.shape, X_train_sem.shape, X_validation_synt.shape, X_validation_sem.shape, X_test_synt.shape, X_test_sem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_data = TensorDataset(X_train_synt, X_train_sem, target_train)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(X_validation_synt, X_validation_sem, target_validation)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(X_test_synt, X_test_sem, target_test)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoW_model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, 150)\n",
    "        self.fc2 = torch.nn.Linear(150, 50)\n",
    "        self.synth_sem_linear = nn.Linear(50, output_dim)\n",
    "\n",
    "    def forward(self, x_synth):        \n",
    "        x_synth = F.dropout(F.relu(self.fc1(x_synth)), p=0.1)\n",
    "        x_synth = F.dropout(F.relu(self.fc2(x_synth)), p=0.1)\n",
    "        x_tot = self.synth_sem_linear(x_synth)\n",
    "        return x_tot\n",
    "\n",
    "class DTFF_Dario(nn.Module):\n",
    "    def __init__(self, input_dim_dt, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim_dt, 4000)\n",
    "        self.fc2 = torch.nn.Linear(4000, 4000)\n",
    "        self.fc3 = torch.nn.Linear(4000, 2000)\n",
    "        self.synth_sem_linear = nn.Linear(2000, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x_synth):        \n",
    "        x_synth = F.dropout(F.relu(self.fc1(x_synth)), p=0.1)\n",
    "        x_synth = F.dropout(F.relu(self.fc2(x_synth)), p=0.1)\n",
    "        x_synth = F.dropout(F.relu(self.fc3(x_synth)), p=0.1)\n",
    "        x_tot = self.synth_sem_linear(x_synth)\n",
    "        return x_tot   \n",
    "\n",
    "class BoWandDT_(nn.Module):\n",
    "    def __init__(self, input_dim_dt, input_dim_we, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim_dt, 4000)\n",
    "        self.fc2 = torch.nn.Linear(4000, 2000)\n",
    "        #self.fc3 = torch.nn.Linear(4000, 2000)\n",
    "        self.synth_linear = nn.Linear(2000, output_dim)\n",
    "        \n",
    "        self.fc4 = torch.nn.Linear(input_dim_we, 150)\n",
    "        self.fc5 = torch.nn.Linear(150, 50)\n",
    "        self.sem_linear = nn.Linear(50, output_dim)\n",
    "        \n",
    "        self.synth_sem_linear = nn.Linear(output_dim*2, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x_synth, x_sem):        \n",
    "        x_synth = F.dropout(F.relu(self.fc1(x_synth)), p=0.1)\n",
    "        x_synth = F.dropout(F.relu(self.fc2(x_synth)), p=0.1)\n",
    "        #x_synth = F.dropout(F.relu(self.fc3(x_synth)), p=0.1)\n",
    "        x_synth = F.dropout(F.relu(self.synth_linear(x_synth)), p=0.1)\n",
    "        \n",
    "        x_sem = F.dropout(F.relu(self.fc4(x_sem)), p=0.1)\n",
    "        x_sem = F.dropout(F.relu(self.fc5(x_sem)), p=0.1)\n",
    "        x_sem = F.dropout(F.relu(self.sem_linear(x_sem)), p=0.1)\n",
    "        \n",
    "        x_synth_sem = torch.cat((x_synth, x_sem), dim=1)\n",
    "        x_tot = self.synth_sem_linear(x_synth_sem)\n",
    "        return x_tot  \n",
    "    \n",
    "class BoWandDT90_25epochs(nn.Module):\n",
    "    def __init__(self, input_dim_dt, input_dim_we, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim_dt, 4000)\n",
    "        self.fc2 = torch.nn.Linear(4000, 2000)\n",
    "        self.synth_linear = nn.Linear(2000, input_dim_we)\n",
    "        self.synth_sem_linear = nn.Linear(input_dim_we*2, input_dim_we)\n",
    "        self.fc4 = torch.nn.Linear(input_dim_we, 150)\n",
    "        self.fc5 = torch.nn.Linear(150, 50)\n",
    "        self.out = nn.Linear(50, output_dim)\n",
    "\n",
    "    def forward(self, x_synth, x_sem):        \n",
    "        x_synth = F.dropout(F.relu(self.fc1(x_synth)), p=0.1)\n",
    "        x_synth = F.dropout(F.relu(self.fc2(x_synth)), p=0.1)\n",
    "        x_synth = F.dropout(F.relu(self.synth_linear(x_synth)), p=0.1)\n",
    "        \n",
    "        x_synth_sem = torch.cat((x_synth, x_sem), dim=1)\n",
    "        \n",
    "        x_sem = F.dropout(F.relu(self.synth_sem_linear(x_synth_sem)), p=0.1)\n",
    "        x_sem = F.dropout(F.relu(self.fc4(x_sem)), p=0.1)\n",
    "        x_sem = F.dropout(F.relu(self.fc5(x_sem)), p=0.1)\n",
    "        x_tot = self.out(x_sem)\n",
    "        return x_tot\n",
    "\n",
    "class BoWandDT(nn.Module):\n",
    "    def __init__(self, input_dim_dt, input_dim_we, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim_dt, 2000)\n",
    "        self.fc3 = nn.Linear(2000, input_dim_we)\n",
    "        self.synth_linear = nn.Linear(input_dim_we, 100)\n",
    "        \n",
    "        self.fc4 = torch.nn.Linear(input_dim_we, 100)\n",
    "        self.concat = nn.Linear(200, 100)\n",
    "        self.fc5 = torch.nn.Linear(100, 50)\n",
    "        self.out = nn.Linear(50, output_dim)\n",
    "\n",
    "    def forward(self, x_synth, x_sem):        \n",
    "        x_synth = F.dropout(F.relu(self.fc1(x_synth)), p=0.1)\n",
    "        x_synth = F.dropout(F.relu(self.fc3(x_synth)), p=0.1)\n",
    "        x_synth = F.dropout(F.relu(self.synth_linear(x_synth)), p=0.1)\n",
    "        \n",
    "        x_sem = F.dropout(F.relu(self.fc4(x_sem)), p=0.1)\n",
    "        \n",
    "        x_synth_sem = torch.cat((x_synth, x_sem), dim=1)\n",
    "        x_sem = F.dropout(F.relu(self.concat(x_synth_sem)), p=0.2)\n",
    "        x_sem = F.dropout(F.relu(self.fc5(x_sem)), p=0.1)\n",
    "        x_tot = self.out(x_sem)\n",
    "        return x_tot  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_train, counts_train = np.unique(target_train, return_counts = True)\n",
    "unique_validation, counts_validation = np.unique(target_validation, return_counts = True)\n",
    "unique_test, counts_test = np.unique(target_test, return_counts = True)\n",
    "print(counts_train, counts_validation, counts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = BoWandDT(4000, 300, 2)\n",
    "model = BoWandDT90_25epochs(4000, 300, 2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_set = []\n",
    "epochs = 5\n",
    "epoch = 0\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "    model.train()  \n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        b_input_tree, b_input_we, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        target_hat = model(b_input_tree, b_input_we)\n",
    "        \n",
    "        loss = criterion(target_hat, b_labels)\n",
    "        train_loss_set.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    ## VALIDATION\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_tree, b_input_we, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_tree, b_input_we)\n",
    "            \n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "    epoch +=1\n",
    "    \n",
    "    print(\"Epochs: {}\".format(epoch))\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "model.eval()\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_tree, b_input_we, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_tree, b_input_we)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    predictions.append(logits)\n",
    "    \n",
    "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(target_test.numpy(), flat_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
