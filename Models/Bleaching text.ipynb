{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "code is taken and adopted from https://github.com/bplank/bleaching-text/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from emoji import UNICODE_EMOJI\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline , FeatureUnion\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8>]                    # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpPxX/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpPxX/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8<]                    # eyes\n",
    "      [<>]?\n",
    "      |\n",
    "      <[/\\\\]?3                         # heart(added: has)\n",
    "      |\n",
    "      \\(?\\(?\\#?                   #left cheeck\n",
    "      [>\\-\\^\\*\\+o\\~]              #left eye\n",
    "      [\\_\\.\\|oO\\,]                #nose\n",
    "      [<\\-\\^\\*\\+o\\~]              #right eye\n",
    "      [\\#\\;]?\\)?\\)?               #right cheek\n",
    "    )\"\"\"\n",
    "\n",
    "emoticon_re = re.compile(emoticon_string, re.VERBOSE | re.I | re.UNICODE)\n",
    "special=set(['NEWLINE','URL','USER'])\n",
    "\n",
    "#prep for frequency\n",
    "def initFreq(text):\n",
    "    frequency=Counter()\n",
    "    for line in text:\n",
    "#         print(line)\n",
    "        frequency.update([e.lower() for e in line.split(' ') if e not in special])\n",
    "    for token,freq in list(frequency.items()):\n",
    "        frequency[token]=str(freq)\n",
    "#         frequency[token]=str(int(log(freq,10)))\n",
    "    for token in special:\n",
    "        frequency[token.lower()]=token\n",
    "    return frequency\n",
    "\n",
    "def frequency(word, frequency):\n",
    "    return(frequency[word.lower()])\n",
    "\n",
    "def length(word):\n",
    "    return '0' + str(len(word))\n",
    "\n",
    "def lex(word):\n",
    "    return word\n",
    "\n",
    "def punctAgr(token):\n",
    "    emojis = ''\n",
    "    for char in token:\n",
    "        if char in UNICODE_EMOJI:\n",
    "            emojis += 'J'\n",
    "    if emojis != '':\n",
    "        return(emojis)\n",
    "    elif (emoticon_re.search(token) != None):\n",
    "        return('E')\n",
    "    else:\n",
    "        isWord = False\n",
    "        for char in token:\n",
    "            if char.isalpha() or char.isalnum():\n",
    "                isWord = True\n",
    "        if isWord:\n",
    "            beg = ''\n",
    "            for i in range(len(token)):\n",
    "                if token[i].isalpha() or token[i].isalnum():\n",
    "                    break\n",
    "                else:\n",
    "                    beg += 'P'\n",
    "            end = ''\n",
    "            for i in range(len(token)-1, 0, -1):\n",
    "                if token[i].isalpha() or token[i].isalnum():\n",
    "                    break\n",
    "                else:\n",
    "                    end += 'P'\n",
    "            return(beg + 'W' + end)\n",
    "        else:\n",
    "            return('P' * len(token)) \n",
    "\n",
    "def punctCons(token):\n",
    "    emojis = ''\n",
    "    for char in token:\n",
    "        if char in UNICODE_EMOJI:\n",
    "            emojis += char\n",
    "    if emojis != '':\n",
    "        return(emojis)\n",
    "    else:\n",
    "        newWord = ''\n",
    "        lastAlpha = False\n",
    "        for char in token:\n",
    "            if char.isalpha() or char.isalnum():\n",
    "                if not lastAlpha:\n",
    "                    newWord += 'W'\n",
    "                lastAlpha = True\n",
    "            else:\n",
    "                newWord += char\n",
    "                lastAlpha = False\n",
    "        return newWord\n",
    "\n",
    "def shape(token):\n",
    "    packed=''\n",
    "    for char in token:\n",
    "        if char.isupper():\n",
    "            packed+='u'\n",
    "        elif char.islower():  \n",
    "            packed+='l'\n",
    "        elif char.isdigit():\n",
    "            packed+='d'\n",
    "        else:\n",
    "            packed+='x'\n",
    "    return re.sub(r'(.)\\1{2,}',r'\\1\\1',packed)\n",
    "        \n",
    "def vowels(word):\n",
    "    new = ''\n",
    "    for char in word:\n",
    "        if char.lower() in 'euioa':\n",
    "            new += 'v'\n",
    "        elif char.isalpha():\n",
    "            new += 'c'\n",
    "        else:\n",
    "            new += 'o'\n",
    "    return new\n",
    "\n",
    "def bleachAll(text, freq):\n",
    "    newText = ''\n",
    "#     for method in ['frequency', 'length', 'punctAgr', 'punctCons', 'shape', 'vowels','lex']:\n",
    "#     for method in [ 'length', 'punctAgr', 'punctCons', 'shape', 'vowels','lex']:    \n",
    "#     for method in ['frequency', 'length', 'punctAgr', 'punctCons', 'shape', 'vowels']:\n",
    "    for method in [ 'length', 'punctAgr', 'punctCons', 'shape', 'vowels']:\n",
    "#     for method in ['lex']:\n",
    "#         if method:# == 'frequency':\n",
    "            newText += bleachText(text, method, freq)\n",
    "    return newText\n",
    "\n",
    "# to use frequency, you have to run it like this:\n",
    "# freqs = bleaching.initFreq(text)\n",
    "# bleaching.bleachText(text, 'frequency', freqs)\n",
    "# the frequency is then based on the entire dataset\n",
    "\n",
    "#if you do not need the frequency transformation:\n",
    "# bleaching.bleachText(text, 'length', None)\n",
    "def bleachText(text, method, freq):\n",
    "    if method == 'all':\n",
    "        return bleachAll(text, freq)\n",
    "    newText = ''\n",
    "    for line in text.split('\\n'):\n",
    "        for word in line.split(' '):\n",
    "            if word not in special:\n",
    "                if method== 'frequency':\n",
    "                    if freq == None:\n",
    "                        print('ERROR: frequency is used, but no counter is given')\n",
    "                    newText += str(frequency(word, freq)) + ' '\n",
    "                if method== 'all':\n",
    "                    if freq == None:\n",
    "                        print('ERROR: all is used, but no counter is given')\n",
    "                    \n",
    "                    newText += str(frequency(word, freq)) + ' '\n",
    "                elif method== 'length':\n",
    "                    newText += length(word) + ' '\n",
    "                elif method== 'lex':\n",
    "                    newText += lex(word) + ' '\n",
    "                elif method== 'punctAgr':\n",
    "                    newText += punctAgr(word) + ' '\n",
    "                elif method== 'punctCons':\n",
    "                    newText += punctCons(word) + ' '\n",
    "                elif method== 'shape':\n",
    "                    newText += shape(word) + ' ' \n",
    "                elif method== 'vowels':\n",
    "                    newText += vowels(word) + ' '\n",
    "                elif method not in [\"all\",'frequency', 'length', 'lex', 'punctAgr', 'punctCons', 'shape', 'vowels']:\n",
    "                    print('ERROR method ' + method + '  does not exist')\n",
    "            else:\n",
    "                newText += word + ' '\n",
    "        newText += ' '\n",
    "    return newText[:-1]\n",
    "\n",
    "import nltk\n",
    "from sklearn.base import TransformerMixin\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# PREFIX_WORD_NGRAM=\"W:\"\n",
    "PREFIX_WORD_NGRAM=\" \"\n",
    "PREFIX_CHAR_NGRAM=\"C:\"\n",
    "TWEET_DELIMITER = \" NEWLINE \"\n",
    "\n",
    "\n",
    "\n",
    "def get_size_tuple(ngram_str):\n",
    "    \"\"\"\n",
    "    Convert n-gram string to tuple\n",
    "    :param ngram_str:  \"1-3\" (lower and upper bound separated by hyphen)\n",
    "    :return: tuple\n",
    "    >>> get_size_tuple(\"3-5\")\n",
    "    (3, 5)\n",
    "    >>> get_size_tuple(\"1\")\n",
    "    (1, 1)\n",
    "    \"\"\"\n",
    "    if \"-\" in ngram_str:\n",
    "        lower, upper = ngram_str.split(\"-\")\n",
    "        lower = int(lower)\n",
    "        upper = int(upper)\n",
    "    else:\n",
    "        lower = int(ngram_str)\n",
    "        upper = lower\n",
    "    return (lower, upper)\n",
    "\n",
    "class Featurizer(TransformerMixin):\n",
    "    \"\"\"Our own featurizer: extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        for all tweets of a user\n",
    "        \"\"\"\n",
    "        out= [self._ngrams(tweets) for tweets in X]\n",
    "        return out\n",
    "\n",
    "    def __init__(self,word_ngrams=\"1\",char_ngrams=\"0\",binary=True,rm_user_url=False):\n",
    "        \"\"\"\n",
    "        binary: whether to use 1/0 values or counts\n",
    "        lowercase: convert text to lowercase\n",
    "        remove_stopwords: True/False\n",
    "        \"\"\"\n",
    "        self.data = [] # will hold data (list of dictionaries, one for every instance)\n",
    "        self.binary=binary\n",
    "        self.word_ngram_size = get_size_tuple(word_ngrams)\n",
    "        self.char_ngram_size = get_size_tuple(char_ngrams)\n",
    "\n",
    "        self.rm_user_url=rm_user_url\n",
    "\n",
    "\n",
    "    def _ngrams(self,tweets):\n",
    "        \"\"\"\n",
    "        extracts word or char n-grams\n",
    "        range defines lower and upper n-gram size\n",
    "        >>> f=Featurizer(word_ngrams=\"1-3\")\n",
    "        >>> d = f._ngrams(\"this is a test\")\n",
    "        >>> len(d)\n",
    "        9\n",
    "        >>> f=Featurizer(word_ngrams=\"0\", char_ngrams=\"2-4\")\n",
    "        >>> d2 = f._ngrams(\"this\")\n",
    "        >>> len(d2)\n",
    "        6\n",
    "        \"\"\"\n",
    "\n",
    "        d={} # new dictionary that holds features for current instance\n",
    "\n",
    "        tweets = tweets.split(TWEET_DELIMITER)\n",
    "\n",
    "        lower, upper = self.word_ngram_size\n",
    "        if lower != 0:\n",
    "            for n in range(lower,upper+1):\n",
    "                for tweet in tweets:\n",
    "                    if self.rm_user_url:\n",
    "                        tweet=tweet.replace(\"USER\",\"\")\n",
    "                        tweet=tweet.replace(\"URL\",\"\")\n",
    "\n",
    "                    ## word n-grams\n",
    "                    for gram in nltk.ngrams(tweet.split(\" \"), n):\n",
    "                        gram = \"{}{}\".format(PREFIX_WORD_NGRAM, \" \".join(gram))\n",
    "                        if self.binary:\n",
    "                            d[gram] = 1 #binary\n",
    "                        else:\n",
    "                            d[gram] = d.get(gram,0)+1\n",
    "\n",
    "        c_lower, c_upper = self.char_ngram_size\n",
    "        if c_lower != 0:\n",
    "            for n in range(c_lower, c_upper + 1):\n",
    "                for tweet in tweets:\n",
    "                    if self.rm_user_url:\n",
    "                        tweet = tweet.replace(\"USER\", \"\")\n",
    "                        tweet = tweet.replace(\"URL\", \"\")\n",
    "\n",
    "                    ## char n-grams\n",
    "                    for gram in nltk.ngrams(tweet, n):\n",
    "                        gram = \"{}_{}\".format(PREFIX_CHAR_NGRAM, \"_\".join(gram))\n",
    "                        if self.binary:\n",
    "                            d[gram] = 1  # binary\n",
    "                        else:\n",
    "                            d[gram] = d.get(gram, 0) + 1\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/surface_web.csv')\n",
    "\n",
    "df.head()\n",
    "\n",
    "df_surface = pd.read_csv(\"./data/surface_web_Forum.csv\")\n",
    "\n",
    "df_dark = pd.read_csv(\"./data/DarkWeb_Covid_Forum.csv\")\n",
    "\n",
    "df_surface['target'] = [0]*len(df_surface)\n",
    "df_dark['target'] = [1]*len(df_dark)\n",
    "\n",
    "list_df = [df_surface, df_dark]\n",
    "df = pd.concat(list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Forum    1260\n",
       "Name: Main_Class, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Main_Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences = df.text.values\n",
    "\n",
    "sentences = df.Text.values\n",
    "\n",
    "\n",
    "#sentences = df.text.values\n",
    "\n",
    "sentences = df.Text.values\n",
    "\n",
    "\n",
    "target_clean_train = []\n",
    "\n",
    "for x in df.target:\n",
    "    if x == 'Forum' or x == 0:\n",
    "        target_clean_train.append(0)\n",
    "    if x == 'Market' or x == 1:\n",
    "        target_clean_train.append(1)\n",
    "        \n",
    "labels = np.array(target_clean_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02 03 06 06 05 W W W WPP W W W W W!? W$W dl ull ull ullxx uuxdd oc ccc cvcvcv cvcvoo vcooo \n"
     ]
    }
   ],
   "source": [
    "random_state = 1024\n",
    "\n",
    "X_inputs, test_inputs, X_labels, test_labels = train_test_split(sentences, labels, random_state=random_state, test_size=0.55)\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(X_inputs, X_labels, random_state=random_state, test_size=0.01)\n",
    "\n",
    "\n",
    "\n",
    "sentences_train = train_inputs\n",
    "labels_train = train_labels\n",
    "\n",
    "\n",
    "sentences_test = test_inputs\n",
    "labels_test = test_labels\n",
    "\n",
    "\n",
    "freqs = initFreq(sentences_test)\n",
    "\n",
    "# print(len(labels_train))\n",
    "# print(df_train.text[218][:20])\n",
    "# print(bleachText(df_train.text[218][:20],\"all\", freqs))\n",
    "print(bleachText(\"1x Pcs Mobile Case!? US$65\",\"all\", None))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02 03 07 02 04 01 08 06 07 03 05 03 03 07 010 08 W W W W W W WP W W WP W W W W PW WP W W W W W W W, W W W. W W W W /W/W W. ul ll ll ll ll l llx ll ll llx ull ll ll ll xlxuudd llx vc cvv cvcvvcv cv cvcv v cvccvcvo ccvvcv cvccvcc vco ccvcc cvv cvc cvvcvcc ococvcvcoo cvccvvco '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bleacher(input_data):\n",
    "    temp = []\n",
    "    for text in input_data:\n",
    "        temp.append(bleachText(text,\"all\", freqs))\n",
    "    return temp\n",
    "\n",
    "bleached_train = bleacher(sentences_train)\n",
    "# bleached_valid = bleacher(validation.text)\n",
    "bleached_test = bleacher(sentences_test)\n",
    "bleached_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9292929292929293\n",
      "0.9292929292929293\n",
      " W\n",
      " \n",
      " ccvcccvc\n",
      " cvccvcvvc\n",
      " cvcccvcv\n",
      " ddxdd\n",
      " W-W\n",
      " vccvccc\n",
      " “W\n",
      " ccvcccvccvvc\n",
      " cvcvcvcvvcc\n",
      " –\n",
      " vccvcco\n",
      " vccc\n",
      " cvvccvc\n"
     ]
    }
   ],
   "source": [
    "n_gram_size = \"1\"\n",
    "\n",
    "dictVectorizer = DictVectorizer()\n",
    "vectorizerWords = Featurizer(word_ngrams=n_gram_size, binary= True)\n",
    "X_train_dict = vectorizerWords.fit_transform(bleached_train)\n",
    "X_test_dict = vectorizerWords.fit_transform(bleached_test)\n",
    "\n",
    "X_train = dictVectorizer.fit_transform(X_train_dict)\n",
    "X_test = dictVectorizer.transform(X_test_dict)\n",
    "# print(X_train)\n",
    "\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train, labels_train)\n",
    "\n",
    "Yguess = classifier.predict(X_test)\n",
    "print(accuracy_score(labels_test, Yguess))\n",
    "\n",
    "\n",
    "\n",
    "lowest = sorted(zip(classifier.coef_[0], dictVectorizer.get_feature_names()))[:100]\n",
    "highest = sorted(zip(classifier.coef_[0], dictVectorizer.get_feature_names()), reverse=True)[:100]\n",
    "feats = dict([(e[1], {}) for e in highest + lowest])\n",
    "# print(feats)\n",
    "# extract_feats(\"0\", True, feats, sentences_train,sentences_train)\n",
    "\n",
    "\n",
    "print(accuracy_score(labels_test, Yguess))\n",
    "for i,j in highest[:15]:\n",
    "#     print(f\"{i:.2f}===> {j}\")\n",
    "    print(f\"{j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_size = \"5\"\n",
    "\n",
    "tfidfVectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,1), lowercase=False)\n",
    "X_train_tf = tfidfVectorizer.fit_transform(bleached_train).toarray()\n",
    "X_test_tf = tfidfVectorizer.transform(bleached_test).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9105339105339105\n",
      "0.9105339105339105\n",
      "cvccvcvcvvc\n",
      "vccvcc\n",
      "cvcvccvc\n",
      "cvcvcvvcc\n",
      "ullx\n",
      "vccvcvcvco\n",
      "cvcccvcv\n",
      "cvcvcvcvvcc\n",
      "cvvcccvvc\n",
      "vccvcvcvcc\n",
      "ccvcccvc\n",
      "cvccvv\n",
      "ccvvc\n",
      "ccv\n",
      "xllx\n",
      "vcccvcvccv\n",
      "cvcvc\n",
      "cvccvcvvc\n",
      "vcv\n",
      "uux\n"
     ]
    }
   ],
   "source": [
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train_tf, labels_train)\n",
    "\n",
    "Yguess = classifier.predict(X_test_tf)\n",
    "print(accuracy_score(labels_test, Yguess))\n",
    "\n",
    "\n",
    "lowest = sorted(zip(classifier.coef_[0], tfidfVectorizer.get_feature_names()))[:100]\n",
    "highest = sorted(zip(classifier.coef_[0], tfidfVectorizer.get_feature_names()), reverse=True)[:100]\n",
    "feats = dict([(e[1], {}) for e in highest + lowest])\n",
    "# print(feats)\n",
    "# extract_feats(\"0\", True, feats, sentences_train, sentences_train)\n",
    "\n",
    "\n",
    "print(accuracy_score(labels_test, Yguess))\n",
    "for i,j in highest[:20]:\n",
    "#     print(f\"{i:.2f}===> {j}\")\n",
    "    print(f\"{j}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_feats(c_n_gram,rm_uu, feats,original,transformed,n_gram =n_gram_size):\n",
    "    \"\"\"\n",
    "    get mapping between transformed and original features (to later extract most frequent ones)\n",
    "    (assumes same strategy as in Featurizer)\n",
    "    \"\"\"\n",
    "    lower, upper = get_size_tuple(n_gram)\n",
    "    if c_n_gram != \"0\":\n",
    "        c_lower, c_upper = get_size_tuple(c_n_gram)\n",
    "    for otext,ttext in zip(original,transformed):\n",
    "        for otweet,ttweet in zip(otext.split(TWEET_DELIMITER),ttext.split(TWEET_DELIMITER)):\n",
    "            if len(otweet.split(' ')) != len(ttweet.split(' ')):\n",
    "                print('DIFF LEN:', otweet, ttweet)\n",
    "                continue\n",
    "            # word n-grams\n",
    "            for n in range(lower, upper + 1):\n",
    "                if rm_uu:\n",
    "                    otweet = otweet.replace(\"USER\", \"\")\n",
    "                    otweet = otweet.replace(\"URL\", \"\")\n",
    "                    ttweet = ttweet.replace(\"USER\", \"\")\n",
    "                    ttweet = ttweet.replace(\"URL\", \"\")\n",
    "                for ongram,tngram in zip(nltk.ngrams(otweet.split(\" \"), n),nltk.ngrams(ttweet.split(\" \"), n)):\n",
    "                    #print(ongram, tngram)\n",
    "                    tngram = \"{}_{}\".format(PREFIX_WORD_NGRAM, \" \".join(tngram))\n",
    "                    ongram=' '.join(ongram).lower()\n",
    "                    if tngram in feats:\n",
    "                        feats[tngram][ongram]=feats[tngram].get(ongram,0)+1\n",
    "            if c_n_gram != \"0\":\n",
    "                ## character n-grams\n",
    "                for n in range(c_lower, c_upper + 1):\n",
    "                    if rm_uu:\n",
    "                        otweet = otweet.replace(\"USER\", \"\")\n",
    "                        otweet = otweet.replace(\"URL\", \"\")\n",
    "                        ttweet = ttweet.replace(\"USER\", \"\")\n",
    "                        ttweet = ttweet.replace(\"URL\", \"\")\n",
    "                    for ongram, tngram in zip(nltk.ngrams(otweet, n), nltk.ngrams(ttweet, n)):\n",
    "                        # print(ongram, tngram)\n",
    "                        tngram = \"{}_{}\".format(PREFIX_CHAR_NGRAM, \"_\".join(tngram))\n",
    "                        ongram = ' '.join(ongram).lower()\n",
    "                        if tngram in feats:\n",
    "                            feats[tngram][ongram] = feats[tngram].get(ongram, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-0c797a647b0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhighest\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlowest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# print(feats)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mextract_feats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentence_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train, labels_train)\n",
    "Yguess = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "lowest = sorted(zip(classifier.coef_[0], dictVectorizer.get_feature_names()))[:100]\n",
    "highest = sorted(zip(classifier.coef_[0], dictVectorizer.get_feature_names()), reverse=True)[:100]\n",
    "feats = dict([(e[1], {}) for e in highest + lowest])\n",
    "# print(feats)\n",
    "extract_feats(\"0\", True, feats, sentences_train, sentence_train)\n",
    "\n",
    "\n",
    "output_dir= \"aria.txt\"\n",
    "OUT = open(output_dir, \"w\")\n",
    "for (coef1, pattern1), (coef2, pattern2) in zip(lowest, highest):\n",
    "    OUT.write(str(coef1) + ' ' + pattern1)\n",
    "    OUT.write((80 - len(str(coef1) + ' ' + pattern1)) * ' ')\n",
    "    OUT.write(str(coef2) + ' ' + pattern2 + '\\n')\n",
    "#     print(coef1)\n",
    "    for (a1, b1), (a2, b2) in zip(sorted(feats[pattern1].items(), key=lambda x: -x[1])[:100],\n",
    "                                  sorted(feats[pattern2].items(), key=lambda x: -x[1])[:100]):\n",
    "#         print(a1,a2)\n",
    "        OUT.write(a1 + (80 - len(a1)) * ' ')\n",
    "        OUT.write(a2 + '\\n')\n",
    "    OUT.write('\\n')\n",
    "\n",
    "print(accuracy_score(labels_test, Yguess))\n",
    "for i,j in highest[:15]:\n",
    "    print(f\"{i:.2f}===>{j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9264069264069265\n"
     ]
    }
   ],
   "source": [
    "# print(train.text)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# tf_idf = CountVectorizer()\n",
    "tf_idf = TfidfVectorizer()\n",
    "X_train_tf = tf_idf.fit_transform(sentences_train).toarray()\n",
    "X_test_tf = tf_idf.transform(sentences_test).toarray()\n",
    "\n",
    "# print(X)\n",
    "# X_train_counts = count_vect.fit_transform(train)#.toarray()\n",
    "\n",
    "# train = X_train_counts\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train_tf, labels_train)\n",
    "\n",
    "Yguess = classifier.predict(X_test_tf)\n",
    "print(accuracy_score(labels_test, Yguess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.44===>covid\n",
      "1.40===>mask\n",
      "1.29===>face\n",
      "0.94===>quality\n",
      "0.87===>protection\n",
      "0.82===>anti\n",
      "0.81===>kn95\n",
      "0.80===>19\n",
      "0.77===>99\n",
      "0.75===>ear\n"
     ]
    }
   ],
   "source": [
    "highest = sorted(zip(classifier.coef_[0], tf_idf.get_feature_names()), reverse=True)[:100]\n",
    "for i,j in highest[:10]:\n",
    "    print(f\"{i:.2f}===>{j}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
